{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    eos_token = \"[EOS]\"\n",
    "    padding_token = \"[PAD]\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, filename, text_key):\n",
    "        self.load_data(filename, text_key)\n",
    "\n",
    "    def load_data(self, filename, text_key):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.text_key = text_key\n",
    "        self.tweets = self.df[text_key]\n",
    "\n",
    "    def clean_data(self, links=True, hashtags=False, mentions=False):\n",
    "        self.tweets = self.tweets.apply(\n",
    "            lambda x: self.__clean_text(x, links, hashtags, mentions))\n",
    "\n",
    "    def __clean_text(self, text, links, hashtags, mentions):\n",
    "        text = text.lower()\n",
    "        if links:\n",
    "            # text = re.sub(r'http\\S+', '', text)\n",
    "            text = re.sub(r'https?://\\S+', '', text)\n",
    "        if hashtags:\n",
    "            text = re.sub(r'@ [\\w]+', '', text)\n",
    "        if mentions:\n",
    "            text = re.sub(r'# [\\w]+', '', text)\n",
    "        return text\n",
    "\n",
    "    def set_padding_token(self, token: str):\n",
    "        self.padding_token = token\n",
    "\n",
    "    def set_eos_token(self, token: str):\n",
    "        self.eos_token = token\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.tweets.values.to_list()\n",
    "\n",
    "    def get_data_df(self):\n",
    "        return self.tweets\n",
    "\n",
    "    # def get_dataloader(self, tokenizer=None, max_length=280, batch_size=32, shuffle=True, pin_memory=True, num_workers=0):\n",
    "    #     dataset = __TweetDataset(tweets=self.tweets.values.to_list(\n",
    "    #     ), tokenizer=tokenizer, max_length=max_length)\n",
    "\n",
    "    #     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory, num_workers=num_workers)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
